{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vis_filters__map_VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhlyRDZPkDCo"
      },
      "source": [
        "## Como visualizar filtros e mapas de recursos em redes neurais convolucionais\n",
        "\n",
        "Redes neurais de aprendizado profundo são geralmente opacas, o que significa que embora possam fazer previsões úteis e habilidosas, não está claro como ou por que uma determinada previsão foi feita.\n",
        "\n",
        "As redes neurais convolucionais têm estruturas internas projetadas para operar sobre dados de imagem bidimensionais e, como tal, preservam as relações espaciais para o que foi aprendido pelo modelo. Especificamente, os filtros bidimensionais aprendidos pelo modelo podem ser inspecionados e visualizados para descobrir os tipos de recursos que o modelo detectará, e a saída dos mapas de ativação por camadas convolucionais pode ser inspecionada para entender exatamente quais recursos foram detectados para uma determinada entrada imagem.\n",
        "\n",
        "Neste tutorial, você descobrirá como desenvolver visualizações simples para filtros e mapas de recursos em uma rede neural convolucional.\n",
        "\n",
        "Depois de concluir este tutorial, você saberá:\n",
        "\n",
        "Como desenvolver uma visualização para filtros específicos em uma rede neural convolucional.\n",
        "Como desenvolver uma visualização para mapas de recursos específicos em uma rede neural convolucional.\n",
        "Como visualizar sistematicamente mapas de recursos para cada bloco em uma rede neural convolucional profunda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFNmQjD5j1Xp"
      },
      "source": [
        "## Visão geral do tutorial\n",
        "Este tutorial é dividido em quatro partes; eles são:\n",
        "\n",
        "1. Visualizando Camadas Convolucionais\n",
        "2. Modelo VGG de pré-ajuste\n",
        "3. Como visualizar filtros\n",
        "4. Como visualizar mapas de recursos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1psEfTkkdi-"
      },
      "source": [
        "## Visualizando Camadas Convolucionais\n",
        "Os modelos de rede neural são geralmente chamados de opacos. Isso significa que eles não sabem explicar o motivo pelo qual uma decisão ou previsão específica foi feita.\n",
        "\n",
        "As redes neurais convolucionais são projetadas para trabalhar com dados de imagem, e sua estrutura e função sugerem que devem ser menos inescrutáveis ​​do que outros tipos de redes neurais.\n",
        "\n",
        "Especificamente, os modelos são compostos de pequenos filtros lineares e o resultado da aplicação de filtros chamados mapas de ativação, ou mais geralmente, mapas de características (ou de atributos).\n",
        "\n",
        "Filtros e mapas de recursos podem ser visualizados.\n",
        "\n",
        "Por exemplo, podemos projetar e entender pequenos filtros, como detectores de linha. Talvez a visualização dos filtros em uma rede neural convolucional aprendida possa fornecer uma visão de como o modelo funciona.\n",
        "\n",
        "Os mapas de recursos que resultam da aplicação de filtros às imagens de entrada e à saída de mapas de recursos de camadas anteriores podem fornecer uma visão sobre a representação interna que o modelo tem de uma entrada específica em um determinado ponto do modelo.\n",
        "\n",
        "Exploraremos ambas as abordagens para visualizar uma rede neural convolucional neste tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueCUq8h-kvOH"
      },
      "source": [
        "## Modelo VGG de pré-ajuste\n",
        "\n",
        "Precisamos de um modelo para visualizar.\n",
        "\n",
        "Em vez de ajustar um modelo do zero, podemos usar um modelo de classificação de imagem de última geração pré-ajuste.\n",
        "\n",
        "Keras fornece muitos exemplos de modelos de classificação de imagens de bom desempenho desenvolvidos por diferentes grupos de pesquisa para o ImageNet Large Scale Visual Recognition Challenge, ou ILSVRC. Um exemplo é o modelo VGG-16 que obteve os melhores resultados na competição de 2014.\n",
        "\n",
        "Este é um bom modelo a ser usado para visualização porque tem uma estrutura uniforme simples de camadas convolucionais e agrupadas ordenadas em série, é profundo com 16 camadas aprendidas e teve um desempenho muito bom, o que significa que os filtros e mapas de recursos resultantes capturarão recursos úteis . Para obter mais informações sobre este modelo, [clique aqui](https://arxiv.org/abs/1409.1556).\n",
        "\n",
        "Executar o exemplo carregará os pesos do modelo na memória e imprimirá um resumo do modelo carregado.\n",
        "\n",
        "Se for a primeira vez que carrega o modelo, os pesos serão baixados da internet e armazenados em seu diretório pessoal. Esses pesos são de aproximadamente 500 megabytes e o download pode demorar um pouco, dependendo da velocidade de sua conexão com a Internet.\n",
        "\n",
        "Podemos ver que as camadas são bem nomeadas, organizadas em blocos e nomeadas com índices inteiros dentro de cada bloco.\n",
        "\n",
        "Podemos carregar e resumir o modelo VGG16 com apenas algumas linhas de código; por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2QUPDRTlm-3"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "# carrega o modelo\n",
        "model = VGG16()\n",
        "# sumário do modelo\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzWyd-Xymful"
      },
      "source": [
        "## Como visualizar filtros\n",
        "\n",
        "Talvez a visualização mais simples de realizar seja plotar os filtros aprendidos diretamente.\n",
        "\n",
        "Na terminologia da rede neural, os filtros aprendidos são simplesmente pesos, mas por causa da estrutura bidimensional especializada dos filtros, os valores de peso têm uma relação espacial entre si e traçar cada filtro como uma imagem bidimensional é significativo (ou poderia ser).\n",
        "\n",
        "A primeira etapa é revisar os filtros no modelo, para ver com o que temos que trabalhar.\n",
        "\n",
        "O resumo do modelo impresso na seção anterior resume a forma de saída de cada camada, por exemplo, a forma dos mapas de recursos resultantes. Não dá nenhuma ideia da forma dos filtros (pesos) na rede, apenas o número total de pesos por camada.\n",
        "\n",
        "Podemos acessar todas as camadas do modelo por meio da propriedade model.layers .\n",
        "\n",
        "Cada camada possui uma propriedade layer.name , onde as camadas convolucionais têm uma convolução de nomenclatura como o bloco # _conv # , onde o ' # ' é um inteiro. Portanto, podemos verificar o nome de cada camada e pular qualquer uma que não contenha a string ' conv '.\n",
        "\n",
        "Cada camada convolucional possui dois conjuntos de pesos.\n",
        "\n",
        "Um é o bloco de filtros e o outro é o bloco de valores de polarização. Eles são acessíveis por meio da função layer.get_weights () . Podemos recuperar esses pesos e resumir sua forma.\n",
        "\n",
        "O código abaixo imprime uma lista de detalhes da camada, incluindo o nome da camada e a forma dos filtros na camada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W8W1nHHn1-c"
      },
      "source": [
        "# summariza os filtros\n",
        "for layer in model.layers:\n",
        "\t# verifica se é camada convolucional\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# pega os filtros (pesos)\n",
        "\tfilters, biases = layer.get_weights()\n",
        "\tprint(layer.name, filters.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmLucJ4jrR9l"
      },
      "source": [
        "Podemos ver que todas as camadas convolucionais usam filtros 3 × 3, que são pequenos e talvez fáceis de interpretar.\n",
        "\n",
        "Uma preocupação arquitetônica com uma rede neural convolucional é que a profundidade de um filtro deve corresponder à profundidade da entrada do filtro (por exemplo, o número de canais).\n",
        "\n",
        "Podemos ver que para a imagem de entrada com três canais para vermelho, verde e azul, cada filtro tem uma profundidade de três (aqui estamos trabalhando com um formato de último canal). Podemos visualizar um filtro como um gráfico com três imagens, uma para cada canal, ou compactar todas as três em uma imagem de cor única, ou mesmo apenas olhar para o primeiro canal e assumir que os outros canais terão a mesma aparência. O problema é que temos 63 outros filtros que gostaríamos de visualizar.\n",
        "\n",
        "Podemos recuperar os filtros da primeira camada convolucional.\n",
        "\n",
        "Os valores de peso provavelmente serão pequenos valores positivos e negativos centralizados em torno de 0,0.\n",
        "\n",
        "Podemos normalizar seus valores para o intervalo 0-1 para torná-los fáceis de visualizar.\n",
        "\n",
        "Podemos enumerar os primeiros seis filtros dos 64 no bloco e representar graficamente cada um dos três canais de cada filtro.\n",
        "\n",
        "Usamos a biblioteca matplotlib e plotamos cada filtro como uma nova linha de subplots, e cada canal de filtro ou profundidade como uma nova coluna.\n",
        "\n",
        "O código completo de traçar os primeiros seis filtros da primeira camada convolucional oculta no modelo VGG16 está listado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjY_8f4lrTg3"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# recupera os filtros (pesos) da segunda camada (primeira camada convolucional)\n",
        "filters, biases = model.layers[1].get_weights()\n",
        "# normaliza para o intervalo [0,1] para torná-los fáceis de visualizar\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plota os primeiros filtros\n",
        "pyplot.figure(figsize=(10, 10))\n",
        "n_filters, ix = 6, 1\n",
        "for i in range(n_filters):\n",
        "\t# pega o filtro\n",
        "\tf = filters[:, :, :, i]\n",
        "\t# plota cada canal separadamente\n",
        "\tfor j in range(3):\n",
        "\t\tax = pyplot.subplot(n_filters, 3, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plota o canal do filtro em nível de cinza\n",
        "\t\tpyplot.imshow(f[:, :, j], cmap='gray')\n",
        "\t\tix += 1\n",
        "# mostra a figura\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmH2T25WwuFp"
      },
      "source": [
        "Temos então seis linhas de três imagens, ou 18 imagens, uma linha para cada filtro e uma coluna para cada canal\n",
        "\n",
        "Podemos ver que em alguns casos, o filtro é o mesmo em todos os canais (a primeira linha), e em outros, os filtros são diferentes (a última linha).\n",
        "\n",
        "Os quadrados escuros indicam pesos pequenos ou inibitórios e os quadrados claros representam pesos grandes ou excitatórios. Usando essa intuição, podemos ver que os filtros na primeira linha detectam um gradiente de luz no canto superior esquerdo para escuro no canto inferior direito.\n",
        "\n",
        "Embora tenhamos uma visualização, vemos apenas os primeiros seis dos 64 filtros na primeira camada convolucional. É possível visualizar todos os 64 filtros em uma imagem.\n",
        "\n",
        "Infelizmente, isso não muda; se desejarmos começar a olhar para os filtros na segunda camada convolucional, podemos ver que novamente temos 64 filtros, mas cada um tem 64 canais para corresponder aos mapas de recursos de entrada. Para ver todos os 64 canais em uma linha para todos os 64 filtros, seria necessário (64 × 64) 4.096 subtramas nas quais pode ser difícil ver qualquer detalhe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviBhsG2osEc"
      },
      "source": [
        "## Como visualizar mapas de recursos\n",
        "\n",
        "Os mapas de ativação, chamados de mapas de recursos, capturam o resultado da aplicação dos filtros à entrada, como a imagem de entrada ou outro mapa de recursos.\n",
        "\n",
        "A ideia de visualizar um mapa de recursos para uma imagem de entrada específica seria entender quais recursos da entrada são detectados ou preservados nos mapas de recursos. A expectativa seria que os mapas de recursos próximos à entrada detectassem detalhes pequenos ou refinados, enquanto os mapas de recursos próximos à saída do modelo capturassem recursos mais gerais.\n",
        "\n",
        "Para explorar a visualização de mapas de características, precisamos de dados para o modelo VGG16 que pode ser usado para criar ativações. Usaremos uma simples fotografia de um gato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cQf_pzK4xN_"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename = 'cat.1700.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuwmP7XP5Qo9"
      },
      "source": [
        "Em seguida, precisamos de uma ideia mais clara da forma da saída dos mapas de feições por cada uma das camadas convolucionais e o número do índice da camada para que possamos recuperar a saída da camada apropriada.\n",
        "\n",
        "O código abaixo irá enumerar todas as camadas no modelo e imprimir o tamanho de saída ou o tamanho do mapa de recursos para cada camada convolucional, bem como o índice de camada no modelo. è possível ver as mesmas formas de saída que vimos no resumo do modelo, mas neste caso apenas para as camadas convolucionais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twZH0-5S5Wlt"
      },
      "source": [
        "# summarize as formas dos mapas de atributos\n",
        "for i in range(len(model.layers)):\n",
        "\tlayer = model.layers[i]\n",
        "\t# verifica se é uma camada convolucional\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# sumariza a forma apenas das camadas convolucionais\n",
        "\tprint(i, layer.name, layer.output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpDHqSrc6Ocu"
      },
      "source": [
        "Podemos usar essas informações e projetar um novo modelo que é um subconjunto das camadas do modelo VGG16 completo. O modelo teria a mesma camada de entrada do modelo original, mas a saída seria a saída de uma determinada camada convolucional, que sabemos que seria a ativação da camada ou do mapa de feições.\n",
        "\n",
        "Por exemplo, depois de carregar o modelo VGG, podemos definir um novo modelo que produz um mapa de características da primeira camada convolucional (índice 1) da seguinte maneira:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRgY3Cr86tks"
      },
      "source": [
        "from keras.models import Model\n",
        "# redefine o modelo para a saída logo após a primeira camada oculta\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[1].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOcAti6Z7UHj"
      },
      "source": [
        "Fazer uma previsão com este modelo fornecerá o mapa de recursos para a primeira camada convolucional para uma determinada imagem de entrada fornecida. Vamos implementar isso.\n",
        "\n",
        "Após definir o modelo, precisamos carregar a imagem do gato com o tamanho esperado pelo modelo, neste caso, 224 × 224."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhrAY5In7bIU"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "# carrega a imagem com o formato requerido\n",
        "img = load_img('cat.1700.jpg', target_size=(224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-kSkUv970J0"
      },
      "source": [
        "Em seguida, o objeto [PIL](https://pt.wikipedia.org/wiki/Python_Imaging_Library) de imagem precisa ser convertido em um array NumPy de dados de pixel e expandido de um array 3D para um array 4D com as dimensões de [ samples, rows, cols, channels ], onde temos apenas uma amostra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6KjkqFl8zDl"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "from numpy import expand_dims\n",
        "# converte a imagem para um array\n",
        "img = img_to_array(img)\n",
        "# expande as dimensões de modo que ele represente a uma única amostra\n",
        "img = expand_dims(img, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4g-aQN09UGC"
      },
      "source": [
        "Os valores de pixel, então, precisam ser dimensionados apropriadamente para o modelo VGG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srfoE8Sa9VUK"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "# prepara a imagem (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud_Qc-xg9kPz"
      },
      "source": [
        "Agora estamos prontos para obter o mapa de recursos. Podemos fazer isso facilmente chamando a função model.predict () e passando a única imagem preparada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PJVh-hM9o0r"
      },
      "source": [
        "# pega o mapa de atributos para a primeira camada escondida\n",
        "feature_maps = model.predict(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHHMpJ3_99Oa"
      },
      "source": [
        "Sabemos que o resultado será um mapa de recursos com 224x224x64. Podemos plotar todas as 64 imagens bidimensionais como um quadrado de 8 × 8 de imagens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0VCot8-9-3h"
      },
      "source": [
        "# plota todos os 64 mapas num grid (subplot) 8x8\n",
        "pyplot.figure(figsize=(10, 10))\n",
        "square = 8\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "\tfor _ in range(square):\n",
        "\t\t# especifica o subplot e eixo\n",
        "\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plota o canal do filtro \n",
        "\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# mostra a figura\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bew345q7-xQ7"
      },
      "source": [
        "A saída acima mostra todos os 64 mapas de recursos.\n",
        "\n",
        "Podemos ver que o resultado da aplicação dos filtros na primeira camada convolucional são muitas versões da imagem do gato com diferentes características destacadas.\n",
        "\n",
        "Por exemplo, algumas linhas de destaque, outras focam o plano de fundo ou o primeiro plano.\n",
        "\n",
        "E lembre-se: este modelo é muito menor que o modelo VGG16, mas ainda usa os mesmos pesos (filtros) na primeira camada convolucional que o modelo VGG16:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CTb3GBU--74"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuzQk5XT_CeC"
      },
      "source": [
        "Este é um resultado interessante e geralmente corresponde à nossa expectativa. Poderíamos atualizar o exemplo para plotar os mapas de recursos da saída de outras camadas convolucionais específicas.\n",
        "\n",
        "Outra abordagem seria coletar a saída dos mapas de recursos de cada bloco do modelo em uma única passagem e, em seguida, criar uma imagem de cada um.\n",
        "\n",
        "Existem cinco blocos principais na imagem (por exemplo, bloco 1, bloco 2, etc.) que terminam em uma camada de agrupamento. Os índices de camada da última camada convolucional em cada bloco são [2, 5, 9, 13, 17].\n",
        "\n",
        "Podemos definir um novo modelo que possui várias saídas, uma saída de mapa de recursos para cada uma das últimas camadas convolucionais em cada bloco; por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96bVt2bSA_Ga"
      },
      "source": [
        "# recarrega o modelo original\n",
        "model = VGG16()\n",
        "# redefine o modelo com múltiplas saídas\n",
        "ixs = [2, 5, 9, 13, 17]\n",
        "outputs = [model.layers[i+1].output for i in ixs]\n",
        "model = Model(inputs=model.inputs, outputs=outputs)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mt-mSWXCmzI"
      },
      "source": [
        "Fazer uma previsão com este novo modelo resultará em uma lista de mapas de recursos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr9X1QHjCn2v"
      },
      "source": [
        "feature_maps = model.predict(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl2YftaqBy3z"
      },
      "source": [
        "Sabemos que o número de mapas de características (por exemplo, profundidade ou número de canais) em camadas mais profundas é muito mais do que 64, como 256 ou 512. No entanto, podemos limitar o número de mapas de características visualizados em 64 para consistência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5lxx0o7B0RX"
      },
      "source": [
        "square = 8\n",
        "block = 0\n",
        "for fmap in feature_maps:\n",
        "  block = block + 1\n",
        "  print('\\nBloco',block,':')\n",
        "  pyplot.figure(figsize=(10, 10))\n",
        "\t# plot all 64 maps in an 8x8 squares\n",
        "  ix = 1\n",
        "  for _ in range(square):\n",
        "    for _ in range(square):\n",
        "      # specify subplot and turn of axis\n",
        "      ax = pyplot.subplot(square, square, ix)\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "\t\t\t# plot filter channel in grayscale\n",
        "      pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
        "      ix += 1\n",
        "\t# show the figure\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DTkTsQWFe7K"
      },
      "source": [
        "A execução do exemplo resulta em cinco grids que mostram os mapas de recursos dos cinco blocos principais do modelo VGG16.\n",
        "\n",
        "Podemos ver que os mapas de recursos mais próximos da entrada do modelo capturam muitos detalhes finos na imagem e que, à medida que avançamos mais no modelo, os mapas de recursos mostram cada vez menos detalhes.\n",
        "\n",
        "Esse padrão era esperado, pois o modelo abstrai os recursos da imagem em conceitos mais gerais que podem ser usados ​​para fazer uma classificação. Embora não esteja claro na imagem final que o modelo viu um gato, geralmente perdemos a capacidade de interpretar esses mapas de recursos mais profundos.\n",
        "\n"
      ]
    }
  ]
}
